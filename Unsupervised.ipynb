{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "0.9978933323970366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28426\n",
      "           1       0.45      0.38      0.41        55\n",
      "\n",
      "    accuracy                           1.00     28481\n",
      "   macro avg       0.72      0.69      0.71     28481\n",
      "weighted avg       1.00      1.00      1.00     28481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from Resampling import data, upsampled, downsampled, y, outlier_fraction\n",
    "\n",
    "\n",
    "\"\"\"The first unsupervised algorithm I will be trying is the isolation forest \n",
    "algorithm with the original data (cut to 10% of its original size for faster\n",
    "computation). \"\"\"\n",
    "\n",
    "# Re-setting X to the data minus the Class feature\n",
    "X = data.drop('Class', axis=1)\n",
    "\n",
    "# Fitting the model\n",
    "a = IsolationForest(max_samples=len(X), contamination=outlier_fraction).fit(X)\n",
    "\n",
    "# Prediction\n",
    "y_prediction = a.predict(X)\n",
    "\n",
    "y_prediction[y_prediction == 1] = 0  # Valid transactions are labelled as 0.\n",
    "y_prediction[y_prediction == -1] = 1  # Fraudulent transactions are labelled as 1.\n",
    "\n",
    "errors = (y_prediction != y).sum()  # Total number of errors is calculated.\n",
    "\n",
    "print(errors)\n",
    "print(accuracy_score(y_prediction, y))\n",
    "print(classification_report(y_prediction, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10794\n",
      "0.7468455368450678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80     32063\n",
      "           1       0.49      1.00      0.66     10575\n",
      "\n",
      "    accuracy                           0.75     42638\n",
      "   macro avg       0.75      0.83      0.73     42638\n",
      "weighted avg       0.87      0.75      0.76     42638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Now, let's try the same isolation forest algorithm with a randomly upsampled dataset with \n",
    "21326 fraudulent and 21326 valid transactions, where the outlier fraction\n",
    "is .5\"\"\"\n",
    "\n",
    "X = upsampled.drop('Class', axis=1)\n",
    "y = upsampled.Class\n",
    "\n",
    "# New model with 50% contamination because there are equal amounts of fraud and valid\n",
    "b = IsolationForest(max_samples=len(X), contamination=.50).fit(X)\n",
    "\n",
    "# Prediction\n",
    "y_prediction2 = a.predict(X)\n",
    "\n",
    "y_prediction2[y_prediction2 == 1] = 0  # Valid transactions are labelled as 0.\n",
    "y_prediction2[y_prediction2 == -1] = 1  # Fraudulent transactions are labelled as 1.\n",
    "\n",
    "errors2 = (y_prediction2 != y).sum()  # Total number of errors is calculated.\n",
    "\n",
    "print(errors2)\n",
    "print(accuracy_score(y_prediction2, y))\n",
    "print(classification_report(y_prediction2, y))\n",
    "\n",
    "\n",
    "\"\"\"The precision with the normal data set is a little bit less than the precision\n",
    "for the upsampled data. The recall with the normal dataset was only around \n",
    "30% (false negatives), while the recall for the upsampled dataset was much better,\n",
    "at around 65%. \n",
    "This shows that up sampling the data before using isolation forest results in\n",
    "better accuracy and less false negatives and false positives\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964888873283944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28426\n",
      "           1       0.02      0.02      0.02        55\n",
      "\n",
      "    accuracy                           1.00     28481\n",
      "   macro avg       0.51      0.51      0.51     28481\n",
      "weighted avg       1.00      1.00      1.00     28481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"Next, we will try the local outlier factor algorithm. First, we will use the original\n",
    "dataset (but 1/10 of the original size)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Reset the variables for X and y for the original data\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data.Class # y is output\n",
    "\n",
    "# Initialize a model with 20 neighbors\n",
    "c = LocalOutlierFactor(n_neighbors = 20,contamination = outlier_fraction)\n",
    "# Fit the model\n",
    "y_prediction3 = c._fit_predict(X)\n",
    "y_prediction3[y_prediction3 == 1] = 0 # Valid transactions are labelled as 0.\n",
    "y_prediction3[y_prediction3 == -1] = 1 # Fraudulent transactions are labelled as 1.\n",
    "\n",
    "errors3 = (y_prediction3 != y).sum()\n",
    "print(accuracy_score(y_prediction3,y))\n",
    "print(classification_report(y_prediction3,y))\n",
    "\n",
    "\"\"\"\n",
    "Both the precision and recall are pretty low, meaning that this is not a good algorithm to apply\n",
    "to this dataset. This is partly due to the fact that there are so many more non-fraud data points\n",
    "than fraud data points, so it is difficult to draw a line between the LOF of fraud and non-fraud cases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, I will try the same LOF algorithm, but with the upsampled dataset this time to try to offset\n",
    "some of the errors in precision and accuracy.\n",
    "\"\"\"\n",
    "\n",
    "X = upsampled.drop('Class', axis=1)\n",
    "y = upsampled.Class\n",
    "\n",
    "# Initialize a model with 20 neighbors\n",
    "c = LocalOutlierFactor(n_neighbors = 20,contamination = .5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}